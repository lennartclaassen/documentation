\chapter{Stand der Technik}
\label{chap:tech}

\section{Lokalisation}
\label{chap:mcl}
\red[Die Lokalisation mobiler Systeme in bekannten Umgebungen ist Gegenstand aktueller Forschungsvorhaben.\\
Während die Positionsbestimmung im Freien durch die Verwendung von GPS \\
Durch die ubiquitäre Nutzung von Computern in Form von Smartphones wird die Lokalisation in Gebäuden]\\
Besonders in der Robotik hat sich die Selbstlokalisation mobiler Systeme in den letzten Jahrzehnten zu einem wichtigen Forschungsbereich entwickelt, da sie eine Voraussetzung für die autonome Navigation von Robotersystemen darstellt. \red[Andere Forschungsbereiche?] Zu unterscheiden ist dabei zwischen den Verfahren der globalen und lokalen Lokalisation. Bei der globalen Lokalisation soll die absolute Pose\footnote{Die Pose eine Systems umfasst die Beschreibung seiner Position und Orientierung bezogen auf die sechs räumlichen Freiheitsgrade \red[Formel oder Quelle?]} des Systems innerhalb seiner Umgebung ermittelt werden. Die lokale Lokalisation bezieht sich hingegen auf die Bestimmung einer relativen Transformation zwischen dem vorhergehenden und dem aktuellen Zustand. Während die globale Lokalisation damit meist angewendet wird um die initiale Pose z. B. in einer bekannten Karte festzulegen dient die lokale Lokalisation dazu die Veränderungen der Systempose kontinuierlich zu verfolgen.\\
Es werden je nach Anwendungsfall dabei unterschiedliche Ansätze angewendet um eine zuverlässige Lokalisation zu realisieren. Im Folgenden soll ein Überblick über die verbreitetsten Ansätze gegeben werden. Eine weitere Unterscheidung innerhalb dieser Ansätze wird dabei nach der Anzahl der zur Lokalisation betrachteten Posen vorgenommen werden. 
%Unimodale Ansätze berücksichtigen jeweils nur eine Pose, während bei multimodalen Verfahren mehrere Posen gleichzeitig aufrechterhalten werden.
\subsection{Unimodale Lokalisationsverfahren}
Bei den unimodalen Verfahren wird nur eine Pose des Systems betrachtet, von welcher ausgehend dann die Bestimmung der absoluten oder relativen Lokalisation erfolgt. Da der Einsatz im Rahmen einer globalen Lokalisation damit meist nur bei Vorlage einer \red[(sinnvollen)] Hypothese eines Anfangszustands praktikabel ist, werden unimodale Verfahren vornehmlich für die lokale Lokalisation eingesetzt. \red[Darauf eingehen, welche Verfahren im Rahmen dieser Arbeit interessant sind/warum einige nicht behandelt werden sollen]

\begin{itemize}

\item Scan Matching: Scan matching bezeichnet den Abgleich von Messungen der Umgebungen mit vorangegangenen Messungen \cite{Gutmann1996} oder anderen Vergleichsdaten wie. beispielsweise einer zuvor aufgezeichneten Karte \cite{Gutmann1998}. Durch Bestimmung der maximalen Überlappung kann die translatorische und rotatorische Veränderung der Pose bezüglich der Referenz berechnet werden. Die betrachteten Messwerte sind dabei meist Distanzen, welche beispielsweise mit Laser- \cite{Diosi2007} oder Ultraschall-Entfernungsmessern \cite{Burguera2005} aufgezeichnet wurden. Die Berechnung der Transformationsvorschrift zwischen den Zuständen erfolgt auf Basis verschiedener Algorithmen wie dem \textit{Iterative Closest Point} Verfahren (ICP) \cite{Besl1992}\cite{Lu1994} oder der \textit{Normal Distributions Transformation} (NDT) \cite{Biber2003}.

\item Linienmatching: Ein das scan matching erweiternder Ansatz ist das line matching, welches die Tatsache ausnutzt, dass Innenräume häufig geradlinige Strukturen aufweisen. Die Robustheit der Lokalisation kann somit gesteigert werden indem anstelle der einzelnen Messpunkte Linien aufeinander abgeglichen werden. Die Linien werden dabei aus den Messwerten extrahiert und mit zuvor erstellten Liniendaten aus Umgebungskarten verglichen \cite{Cox1991}\cite{Gutmann1999}. Auch ein umgekehrtes Vorgehen ist möglich, in welchem die Liniendaten der Karten in äquidistante Punktmengen transformiert und mit den Daten der Entfernungsmessung abgeglichen werden. In diesem Fall reduziert sich die Berechnung der Überdeckung auf ein abgleichen von Punkten, welches mit den Algorithmen des scan matching gelöst werden kann. Eine Erhöhung der Komplexität wird durch die Betrachtung von Polylinien, also zusammenhängenden Linienelementen erreicht \cite{Wolter2004}. Zur Lösung können dabei Algorithmen angewendet werden, welche für das Abgleichen von Formen entwickelt wurden und ursprünglich aus der computergestützen Bildverarbeitung stammen.\\
%Ähnlich wie das Scanmatching von Punkten, allerdings werden Linien miteinander verglichen, da besonders in Gebäuden etc. die robustheit aufgrund der natürlich vorhandenen Linien gesteigert wird. Das matching kann dabei auf verschiedenen Arten durchgeführt werden: Extraktion von Linien aus den Messdaten (glättet Daten evtl. auch) und Matching auf Kartendaten, Extraktion von (äquidistanten) Punkten aus Kartendaten und Matchinng auf Punktdaten wie beim Scan Matching oder auch Erstellung von Deskriptoren für linien wie länge, winkel, Mittelpunkt.\\
Eine andere Art von Ansätzen des line matchings verfolgen die Beschreibung der Liniendaten mittels verschiedener Deskriptoren wie Länge, Mittelpunkt und Neigungswinkel \red[RICK, auch für QUELLEN!?]. Dieses Vorgehen ermöglicht \red[XXX] und bildet den Übergang zu den merkmalsbasierten Lokalisationsverfahren.

\item Merkmalsbasiert: Die merkmalsbasierte Lokalisation ist eine allgemeinere Anwendung des Prinzips von Deskriptoren (Features) zur Ermittlung der aktuellen Pose eines Systems. Betrachtet werden je nach Anwendungsfall und verwendeter Sensorik unterschiedlichste Merkmale. Neben Distanzmessungen \cite{Tomono2004} werden insbesondere auch Kamerasysteme verwendet um Merkmale aus der Umgebung zu extrahieren \cite{Se2001}. Durch die hier ebenfalls vorhandene Nähe zu Anwendungsfällen in der computergestützten Bildverarbeitung kann auf eine Vielzahl von Algorithmen aus diesem Forschungsbereich zurückgegriffen werden. Da die Definition geeigneter Deskriptoren außerdem bei der Verwendung von Entfernungsmessern häufig nicht generalisierbar ist \red[belegen!?], hat sich für die merkmalsbasierte Lokalisation die Verwendung von Bilddaten als Basis bewährt und wird unter dem Begriff der visuellen Odometrie\red[Fußnote o.a.] zusammengefasst.\\
%Definition der Deskriptoren bei Laserscan etc. schwierig und Anwendungsabhängig -> hauptsächlicher Einsatz von merkmalsbasierter Lokalisation bei visuellen Daten (Kamerabildern) \red[visuelle Odometrie fällt darunter!] SIFT/SURF, PCA.

\item Kalman Lokalisation: Ein weiteres unimodales Lokalisationsverfahren ist die Schätzung und Verfolgung der Systempose mittels eines Erweiterten Kalman-Filters\red[Fußnote oder Beschreibung im Anhang; Monomodale Variante des Bayes Filters]. Dieses Verfahren kann auf den bisher beschriebenen matching Verfahren aufbauen, ist dabei jedoch nicht auf diese limitiert. Durch das Kalman-Filter wird eine Fusionierung der Sensordaten mit der Odometrie erreicht, wobei die Sensordaten besipielsweise aus globalen Kartenmerkmalen ermittelt werden \cite{Leonard1991} oder aus der Kombination verschiedener Sensoren \cite{Roumeliotis1997} resultieren. Ein großer Vorteil dieses Lokalisationsverfahrens liegt demnach darin, mehrere Datenquellen, sowohl für Odometrie als auch für Messungen integrieren zu können. Da das Kalman-Filter die Pose mittels einer Wahrscheinlichkeitsdichtefunktion annähert eignet er sich besonders unter der Voraussetzung, dass eine Approximation der initialen Pose vorliegt. Die Anwendung in der globalen Lokalisation ohne initiale Startpose ist nur durch parallele Verwendung multipler Kalman-Filter zu realisieren und führt damit zu einem multimodalen Lokalisationsverfahren.
%Anwendung des Kalman-Filters auf mehrere Hypothesen führt zu multimodalen Lokalisationsverfahren. 
%EKF approximiert Verteilung lokal als Gaußverteilung
\end{itemize}
\subsection{Multimodale Lokalisationsverfahren}
Multimodale Lokalisationsverfahren erhalten stets mehrere mögliche Systemposen aufrecht. Dadurch ermöglichen sie neben einer globalen Lokalisation ohne Anfangshypothese auch das Retten aus falschen Lokalisationen in lokalen Minima. Im Falle einer fehlerhaften Lokalisation ist beispielsweise das scan matching zwar in der Lage die Überdeckung zwischen Sensordaten und aktueller Kartenumgebung zu minimieren, der Algorithmus erkennt jedoch nicht, ob sich das System an einer falschen Stelle befindet. Wichtig wird dieser Aspekt insbesondere beim sogenannten \textit{kidnapped robot scenario}, bei welchem das System im Betrieb aus seiner bekannten Pose in eine unbekannten Pose gebracht wird \cite{Yic2011} ohne dabei Sensordaten zu verwerten. Multimodale Verfahren können dem Begegnen, indem sie stets eine Anzahl \textit{n} an wahrscheinlichsten Posen betrachten und/oder in jedem Lokalisationsschritt zufällige Posen in die Betrachtung integrieren.\\
\begin{itemize}
\item Markov Lokalisation: Die Basis der Markov Lokalisation bildet die Diskretisierung des Posenraums. Es wird dabei für jeden Freiheitsgrad eine Rasterkarte erstellt, in welcher die Gitterzellen Repräsentationen möglicher Posen darstellen. Die Wahrscheinlichkeitsdichte wird für jede Zelle auf Basis von Varianten des Bayes Filter bestimmt, zu denen auch das Kalman Filter gehört \cite{Hertzberg2012}. In jedem Lokalisationsschritt werden Odometrie oder Sensordaten verarbeitet und die Wahrscheinlichkeitsdichte der Gitterzellen basierend darauf angepasst. Da selbst bei deutlichem Anstieg der Probabilität um eine Pose auch alle weiteren diskreten Posen weiter betrachtet werden kann es somit nicht passieren, dass das Vertrauen in die Pose mit der aktuell höchsten Wahrscheinlichkeit so groß wird, dass das System nicht in der Lage ist auf falsche Lokalisationen zu reagieren.\\
Die globale Lokalisation erfolgt bei der Markov Lokalisation entweder durch Initialisierung mit einer Gleichverteilung über alle Zellen, oder durch eine Initialisierung mit Normalverteilung und geringer Varianz um die Hypothesen der Startpositionen \cite{Hertzberg2012}.\\

\item Monte Carlo Lokalisation: Die Diskretisierung und gleichzeitige Betrachtung aller möglichen Posen ist mit großem Rechenaufwand verbunden. Die Monte Carlo Lokalisation ist in der Lage dieses Problem zu umgehen, indem es anstelle aller möglichen Posen nur ausgewählte Stichproben betrachtet. Jede dieser Stichproben entspricht einer Pose und wird auch als Partikel bezeichnet. Neben dem Ziehen der Partikel aus einem diskreten Posenraum ist es dadurch ebenso möglich, die Stichproben aus einem kontinuierlichen Posenraum, wie er in der mobilen Robotik vorliegt, zu ziehen \cite{Fox2001}. Die Kontrolle über die Partikelanzahl ermöglicht es zudem, den Algorithmus auf die verfügbaren Rechenressourcen abzustimmen \cite{Thrun2001}. Durch die geringen Einschränkungen bezüglich der als Basis verwendeten Wahrscheinlichkeitsverteilungen ist die Monte Carlo Lokalisation in der Lage die globale und lokale Lokalisation mit hoher Genauigkeit zu realisieren \cite{Thrun2005}. Die hohe Effizienz dieses Ansatzes führt dazu, dass er bei der Lokalisation mobiler Systeme der Markov Lokalisation deutlich überlegen ist \cite{Fox2001}.
\end{itemize}

\red[Monte Carlo etc. ->Buch\\]
\red[smartphone lokalisierung thematisieren aber verwendet externe Sensorik zur Positionsbestimmung]
\red[Viele Forschungsgruppen beschäftigen sich mit der Problematik, sowohl Lokalisation in bekannter als auch unbekannter Umgebung.
Da das Ziel des entwickelten Systems in der Lokalisation in bekannter Umgebung liegt wird sich bei der Darstellung der bisherigen Forschungsansätze auf diesen Bereich konzentriert. Da das entwickelte System aufgrund seiner Komponenten dafür ausgelegt ist, die Lokalisation auf Basis von Tiefen- und Farbinformationen durchzuführen werden an dieser Stelle Ansätze, welche auf anderen Verfahren wie Ortung von Funkwellen basieren, ebenfalls nicht näher ausgeführt.]\\

\red[In der Robotik wird die fortlaufende Ableitung der Orientierung und Geschwindigkeit aus Messungen der Raddrehwinkel als Odometrie bezeichnet. \cite{Hertzberg2012}]

\red[Welche Lokalisationsverfahren gibt es. Allgemein und speziell für handgeführte Systeme.]\\


\section{RGB-D Kameras/Tiefenkameras(?)}
Die beschriebenen Verfahren und Algorithmen haben sich besonders im Bereich der 2D Lokalisation bewährt, obwohl sie prinzipiell unabhängig von der Anzahl an Dimensionen sind. Die größer werdende Verbreitung von \red[bezahlbaren] leistungsfähigen Tiefenkameras\red[für normale Anwender] wie der Microsoft Kinect\red[Tm] führt jedoch dazu, dass die Anwendungen immer häufiger auch auf 3D Umgebungen erweitert werden. Die Lokalisation in 3D Umgebungen ist besonders dann von Bedeutung, wenn das System sich in mehr als einer Ebene bewegen kann. Die Anzahl der Freiheitsgrade (in der 2D Lokalisation meist drei) steigt dadurch auf sechs an, da die Pose des Systems nun über drei translatorische sowie drei rotatorische Freiheitsgerade beschrieben wird. Neben der Notwendigkeit für geeignete 3D Modellumgebungen kann die Erhöhung der Freiheitsgrade auch dazu führen, dass keine Odometriedaten mehr vorhanden sind. Dies ist insbesondere bei fliegenden \cite{Huang2011} und hand- oder körpergeführten Systemen \cite{Fallon2012} der Fall. Die Auswertung von Tiefen- und Farbinformationen ermöglicht es jedoch dies durch visuelle Odometrie zu kompensieren \cite{Whelan2013robust}.\\
Auch die Integration in traditionelle Robotersysteme zeigt, dass Tiefenkameras eine sinnvolle Alternative zu bisherigen Sensoren wie Laser-Entfernungsmessern darstellen können \cite{Cunha2011} \cite{Eriksson2012}. Anzumerken ist, dass der beobachtbare Bildbereich bezüglich Distanz und Sichtfeld meist deutlich kleiner ist, als bei Laser Sensoren. Die Eignung von Tiefenkameras für Lokalisationsaufgaben ist daher anwendungsabhängig zu überprüfen.\\

\red[Featurebasierte Lokalisation (RGB-D SLAM, Fovis)\\
Markerbasierte Lokalisation]\\

\section{Projektion/Augmented Reality}
\red[Projektion von Bildinformationen (Augmented Reality Bereich) ebenfalls großes Forschungsgebiet, besonders im Bereich der digitalen Bildverarbeitung (computer vision).]\\

\red[In welchen Bereichen wird Augmented Reality angewendet und für welche Anwendungen/in welcher Weise soll es den Benutzer unterstützen?\\]

\section{Interaktion}
\red[Benutzerinteraktion basierend auf der Verwendung von Tiefeninformationen. Hauptsächlicher Ansatz ist die Befehlsvorgabe über Gestensteuerung.]\\
\red[Welche Formen von Benutzerinteraktion gibt es, besonders bezogen auf die Kinect und Projektionssysteme.\\
(z.B. Omnitouch)]


\section{Systeme}
\subsection{Lokalisation}
\red[Ein handgeführtes Scanning System, entwickelt von Mair \textit{et al.} \cite{Mair2010}, fusioniert IMU Daten und  \\]

\subsection{Projektion}
Die Navigation innerhalb eines Museums wird von Wecker \textit{et al.} \cite{Wecker2013} mittels handgeführter Projektionssysteme durch die Visualisierung von Karten- und Wegdaten unterstützt.\\
Ein System mit ähnlichem Ziel haben Chung \textit{et al.} \cite{Chung2011} entwickelt, welches dem Anwender bei der Navigation innerhalb von Gebäuden behilflich sein soll. Ein Miniprojektor wird dabei in Kombination mit einem Smartphone verwendet um zusätzliche Informationen bei Erkennung von Visitenkarten oder Gebäudeplänen zu visualisieren. Die Funktionalität soll dabei an eine Taschenlampe erinnern, welche die Zusatzinformationen sichtbar macht.\\
Auch Li \textit{et al.} \cite{Li2013} stellen ein handgeführtes Projektionssystem vor, welches im Konzept an eine Taschenlampe angelehnt ist. Durch Projektion von Karten- und Wegdaten auf den Boden vor dem Benutzer wird dieser entlang eines Weges geführt. Im Gegensatz zu Chung \textit{et al.} erfolgt dabei eine kontinuierliche Aktualisierung der Projektion in Abhängigkeit der Position entlang des Weges. Die Lokalisation wurde dabei manuell durch eine Begleitperson vorgenommen.\\
Molyneaux \textit{et al.} \cite{Molyneaux2012} erweitern die Metapher der Taschenlampe und integrieren darüber hinaus eine automatische Lokalisation des handgeführten Projektionssystems. Eine Infrastruktur aus Microsoft Kinect Sensoren erkennt und verfolgt die Systempose und ermöglicht dadurch die verzerrungsfreie Projektion beliebiger Zusatzinformationen innerhalb eines Raumes. Durch Infrarot Kameras am Projektionssystem selbst wird zudem eine Interaktion mit den visualisierten Daten realisiert.\\
Das \textit{SideBySide} Projekt von Willis \textit{et al.} \cite{Willis2011} ermöglicht die Interaktion von Benutzern über handgeführte Projektionssysteme. Jedes System projiziert dabei sowohl ein Bild im sichtbaren Lichtspektrum als auch einen Marker im Infrarot Spektrum. Die Erkennung der Marker durch die Systeme erlaubt das Zusammenspiel der jeweiligen Projektionen der Anwender. Beispielanwendungen finden sich im Austausch von Informationen oder Dateien und in kooperativen Spielen.\\
Einen weiteren Ansatz für kooperative Projektionssysteme liefern Robinson \textit{et al.} \cite{Robinson2012} mit \textit{PicoTales}. Dabei werden handgeführte Projektionssysteme verwendet um gemeinsam animierte Videos zu erstellen. Die Lokalisation erfolgt dabei nach einem Kalibrierungsverfahren über das Aufzeichnen von Bewegungsdaten durch eine inertiale Messeinheit. Die Auswertung und Fusionierung zu einem gemeinsamen Video erfolgt nach Ende der Interaktion über einen separaten Computer.\\
Das von Harrison \textit{et al.\ }\cite{Harrison2011} entwickelte \textit{Omnitouch} ist ein körpergeführtes System, welches die Projektion grafischer Benutzeroberflächen auf typische im Alltag vorhandenen Oberfläche ermöglicht. Das System verfügt neben einem Projektor auch über eine Tiefenkamera zur Detektion von Benutzereingaben. Dadurch wird die Funktionalität von Touchscreens abgebildet und es können typische Anwendungen implementiert werden, die sonst beispielsweise auf Smartphones oder Tablets genutzt werden.\\

Warum der Ansatz?
\red[Verschiedene handgeführte Systeme, welche jedoch entweder auf manueller, externer Lokalisation, oder markerbasierter Lokalisation beruhen. Bei anderen Systemen wird lediglich eine Ausrichtung bzgl der Projektionsoberfläche durchgeführt. Selbstlokalisation ohne hilfsmittel bisher nicht behandelt. Lokalisation von mobilen Systemen allerdings seit einiger Zeit Forschungsthema, neuer ist jedoch die 3D Lokalisation. System soll die Brücke bilden zwischen mobilen, autonomen Lokalisationssystemen und handgeführten Projektionssystemen\\]
\red[OHNE HILFSMITTEL nochmal aufgreifen wenn alternative Lokalisation über Muster o.ä. aufgeführt wird]

\red[Welche Systeme gibt es zur Projektion von (Modell-)Daten.]\\

%\includesvgnew[1]{test}
