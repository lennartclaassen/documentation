\chapter{Stand der Technik}
\label{chap:tech}
%\red[TODO:\\
%Lokalisation mobiler Systeme in Gebäuden etc.\\
%Augmented Reality inkl. Interaktion mit Projektionen\\
%Handgeführte Lokalisationssysteme (Ohne Projektion)\\
%]
\red[Viele Forschungsgruppen beschäftigen sich mit der Problematik, sowohl Lokalisation in bekannter als auch unbekannter Umgebung.
Da das Ziel des entwickelten Systems in der Lokalisation in bekannter Umgebung liegt wird sich bei der Darstellung der bisherigen Forschungsansätze auf diesen Bereich konzentriert. Da das entwickelte System aufgrund seiner Komponenten dafür ausgelegt ist, die Lokalisation auf Basis von Tiefen- und Farbinformationen durchzuführen werden an dieser Stelle Ansätze, welche auf anderen Verfahren wie Ortung von Funkwellen basieren, ebenfalls nicht näher ausgeführt.]\\


Da sich die geplante Anwendung des \kps{s} in verschiedene Funktionsbereiche gliedern lässt, soll im Folgenden ein Überblick über den aktuellen Forschungsstand innerhalb der jeweiligen Bereiche gegeben werden. Abschließend werden Systeme vorgestellt, welche eine mit dem in dieser Arbeit entwickelten \kps{} vergleichbare Funktionalität in einem oder mehreren Bereichen bieten.

\section{Lokalisation}
\label{chap:mcl}
\red[Die Lokalisation mobiler Systeme in bekannten Umgebungen ist Gegenstand aktueller Forschungsvorhaben.]\\
Aufgrund des Anwendungsgebietes des entwickelten \kps{s} beschränkt sich der folgende Überblick auf die Lokalisation von mobilen Systemen in Innenräumen. Aus diesem Grund werden ebenfalls keine Lokalisationsverfahren betrachtet, die auf der Verwendung von externer Sensorik zur Bestimmung der Systempose basieren.\\
Die Selbstlokalisation mobiler Systeme hat sich besonders in der Robotik in den letzten Jahrzehnten zu einem wichtigen Forschungsbereich entwickelt, da sie eine Voraussetzung für die autonome Navigation von Robotersystemen darstellt \red[Andere Forschungsbereiche?].\\
Zu unterscheiden ist dabei zwischen den Verfahren der globalen und lokalen Lokalisation. Bei der globalen Lokalisation soll die absolute Pose\footnote{Die Pose eine Systems umfasst die Beschreibung seiner Position und Orientierung bezogen auf die sechs räumlichen Freiheitsgrade \red[Formel oder Quelle?]} des Systems innerhalb seiner Umgebung ermittelt werden. Die lokale Lokalisation bezieht sich hingegen auf die Bestimmung einer relativen Transformation zwischen dem vorhergehenden und dem aktuellen Zustand. Während die globale Lokalisation damit meist angewendet wird um die initiale Pose in einer bekannten Umgebung festzulegen dient die lokale Lokalisation dazu die Veränderungen der Systempose kontinuierlich zu verfolgen.\\ \red[Begriff Tracking nennen?\\]
Eine weitere Unterscheidung innerhalb dieser Ansätze wird dabei nach der Anzahl der betrachteten Hypothesen vorgenommen. Während unimodale Lokalisationsverfahren sich auf eine Hypothese zur Bestimmung der Systempose beschränken, werden bei multimodalen Lokalisationsverfahren mehrere Hypothesen parallel betrachtet.
%Im Folgenden soll ein Überblick über verbreitete Lokalisationsansätze gegeben werden.
%Unimodale Ansätze berücksichtigen jeweils nur eine Pose, während bei multimodalen Verfahren mehrere Posen gleichzeitig aufrechterhalten werden.
\subsection{Unimodale Lokalisationsverfahren}
Da bei unimodalen Verfahren nur eine mögliche Pose des Systems betrachtet wird, erfordert der Einsatz im Rahmen einer globalen Lokalisation damit meist die \red[(sinnvolle)] Hypothese eines Anfangszustands. Vornehmliche Anwendung finden sie daher im Bereich der lokalen Lokalisation. \red[Darauf eingehen, welche Verfahren im Rahmen dieser Arbeit interessant sind/warum einige nicht behandelt werden sollen]

Einen vergleichsweise einfachen Ansatz stellt das sogenannte Scan Matching \red[Wie kenntlich machen? Quotes oder kursiv? Fett?] dar. Dieses Verfahren bezeichnet den Abgleich \red[(Matching)] von Messungen der Umgebungen mit vorangegangenen Messungen \cite{Gutmann1996} oder zuvor aufgezeichneten Vergleichsdaten \cite{Gutmann1998}. Durch Bestimmung der maximalen Überlappung kann die translatorische und rotatorische Veränderung der Pose bezüglich der Referenz berechnet werden. Die betrachteten Messwerte sind dabei meist Distanzen, welche beispielsweise mit Laser- \cite{Diosi2007} oder Ultraschall-Entfernungsmessern \cite{Burguera2005} aufgezeichnet wurden. Die Berechnung der Transformationsvorschrift zwischen den Zuständen erfolgt auf Basis verschiedener Algorithmen wie dem \textit{Iterative Closest Point} Verfahren (ICP) \cite{Besl1992}\cite{Lu1994} oder der \textit{Normal Distributions Transformation} (NDT) \cite{Biber2003}.\\

Das \red[Line Matching] erweitert diesen Ansatz, indem es die häufig geradlinige Struktur von Innenräumen nutzt. Anstelle einzelner Messpunkte werden Linien aufeinander abgeglichen, wodurch die Robustheit der Lokalisation gesteigert werden kann. Die Linien werden dabei aus den Messwerten extrahiert und mit zuvor erstellten Liniendaten aus Umgebungskarten verglichen \cite{Cox1991}\cite{Gutmann1999}. Auch ein umgekehrtes Vorgehen ist möglich, in welchem die Liniendaten der Karten in äquidistante Punktmengen transformiert und mit den Daten der Entfernungsmessung abgeglichen werden. In diesem Fall reduziert sich die Berechnung der Überdeckung auf ein abgleichen von Punkten, welches mit den Algorithmen des Scan Matching gelöst werden kann.\\
Eine Erhöhung der Komplexität wird durch die Betrachtung von Polylinien, also zusammenhängenden Linienelementen erreicht \cite{Wolter2004}. Zur Lösung können dabei Algorithmen angewendet werden, welche für das Abgleichen von Formen entwickelt wurden und ursprünglich aus der computergestützen Bildverarbeitung stammen.\\
Eine andere Art von Ansätzen des Line Matchings verfolgt die Beschreibung der Liniendaten mittels verschiedener Deskriptoren wie Länge, Abstand und Winkel \cite{Frey2014} \cite{Garulli2005}. Dieses Vorgehen ermöglicht eine kompaktere Darstellung der relevanten Umgebungsmerkmale und bildet den Übergang zu den merkmalsbasierten Lokalisationsverfahren.\\

Merkmalsbasierte Lokalisation beschreibt eine allgemeinere Anwendung des Prinzips von Deskriptoren (Features) zur Ermittlung der aktuellen Pose eines Systems. Betrachtet werden je nach Anwendungsfall und verwendeter Sensorik unterschiedlichste Merkmale. Neben Distanzmessungen \cite{Tomono2004} werden insbesondere auch Kamerasysteme verwendet um Merkmale aus der Umgebung zu extrahieren \cite{Se2001}. Durch die hier ebenfalls vorhandene Nähe zu Anwendungsfällen in der computergestützten Bildverarbeitung kann auf eine Vielzahl von Algorithmen aus diesem Forschungsbereich zurückgegriffen werden. Da die Definition geeigneter Deskriptoren darüber hinaus bei der Verwendung von Entfernungsmessern häufig nicht generalisierbar ist \red[belegen!?], hat sich für die merkmalsbasierte Lokalisation die Verwendung von Bilddaten als Basis bewährt und wird unter dem Begriff der visuellen Odometrie \cite{Mccarthy2003} zusammengefasst.\\
%Definition der Deskriptoren bei Laserscan etc. schwierig und Anwendungsabhängig -> hauptsächlicher Einsatz von merkmalsbasierter Lokalisation bei visuellen Daten (Kamerabildern) \red[visuelle Odometrie fällt darunter!] SIFT/SURF, PCA.

Ein weiteres unimodales Lokalisationsverfahren ist die Schätzung und Verfolgung der Systempose mittels eines Kalman-Filters\red[Fußnote oder Verweis auf Beschreibung später?; Monomodale Variante des Bayes Filters]. Dieses Verfahren kann auf den bisher beschriebenen Matching Verfahren aufbauen, ist dabei jedoch nicht auf diese limitiert. Durch das Kalman-Filter wird eine Fusionierung der Sensordaten mit der Odometrie \red[schon beschrieben?] erreicht, wobei die Sensordaten beispielsweise aus globalen Kartenmerkmalen ermittelt werden \cite{Leonard1991} oder aus der Kombination verschiedener Sensoren \cite{Roumeliotis1997} resultieren. Ein großer Vorteil dieses Lokalisationsverfahrens liegt somit darin, mehrere Datenquellen in einem Modell vereinen zu können.\\ Da das Kalman-Filter die Pose mittels einer Wahrscheinlichkeitsdichtefunktion annähert eignet er sich besonders unter der Voraussetzung, dass eine Approximation der initialen Pose vorliegt. Als globales Lokalisationsverfahren bei unbekannter Startpose eignet sich der Kalman-Filter dagegen nur bedingt. Diese Limitierung kann jedoch durch parallele Verwendung multipler Kalman-Filter, also dem Einsatz eines multimodalen Lokalisationsverfahren, überwunden werden.
%Anwendung des Kalman-Filters auf mehrere Hypothesen führt zu multimodalen Lokalisationsverfahren. 
%EKF approximiert Verteilung lokal als Gaußverteilung
\subsection{Multimodale Lokalisationsverfahren}
Multimodale Lokalisationsverfahren erhalten stets mehrere Hypothesen möglicher Systemposen aufrecht. Dadurch ermöglichen sie neben einer globalen Lokalisation ohne Anfangshypothese auch das Wiedererlangen richtiger Posen nach fälschlicher Lokalisation in lokalen Minima. Im Falle einer fehlerhaften Lokalisation ist beispielsweise das Scan Matching zwar in der Lage die Überdeckung zwischen Sensordaten und lokaler Kartenumgebung zu minimieren, der Algorithmus erkennt jedoch nicht, ob sich das System global betrachtet an einer falschen Stelle befindet. Wichtig wird dieser Aspekt insbesondere beim sogenannten \textit{kidnapped robot scenario}, bei welchem das System im Betrieb aus seiner bekannten Pose in eine unbekannten Pose gebracht wird \cite{Yic2011} ohne dabei Sensordaten zu verwerten. Multimodale Verfahren können dem begegnen, indem sie stets eine Anzahl \textit{n} an Posen mit der höchsten Plausibilität betrachten und in jedem Lokalisationsschritt zufällige Posen in die Betrachtung integrieren.\\

Das bei der Markov Lokalisation angewendete Prinzip basiert auf einer Diskretisierung des Posenraums. Für jeden Freiheitsgrad des Systems wird eine Rasterkarte erstellt, in welcher die Gitterzellen mögliche Posen innerhalb des diskreten Raumes repräsentieren. Jede Hypothese wird über eine Wahrscheinlichkeitsdichte abgebildet, welche auf Basis von Varianten des Bayes Filter \red[erklären? wo?] bestimmt wird, zu denen auch das Kalman Filter gehört \cite{Hertzberg2012}. In jedem Lokalisationsschritt werden Odometrie- \red[schon erklärt?] und Sensordaten verarbeitet und die Wahrscheinlichkeitsdichte der Gitterzellen basierend darauf angepasst. Da selbst bei deutlichem Anstieg der Probabilität einer Pose auch die weiteren diskreten Posen in der Betrachtung erhalten bleiben, ist das System stets in der Lage auf falsche Lokalisationen zu reagieren.\\
Die globale Lokalisation erfolgt bei der Markov Lokalisation entweder durch Initialisierung mit einer Gleichverteilung über alle Zellen, oder durch eine Initialisierung mit Normalverteilung und geringer Varianz um die Hypothesen der Startpositionen \cite{Hertzberg2012}.\\

Die Diskretisierung und gleichzeitige Betrachtung aller möglichen Posen ist mit großem Rechenaufwand verbunden. Die Monte Carlo Lokalisation ist in der Lage dieses Problem zu umgehen, indem es anstelle aller möglichen Posen nur ausgewählte Stichproben betrachtet. Jede dieser Stichproben entspricht einer Pose und wird auch als Partikel bezeichnet. Die Partikel können dabei neben dem diskreten Posenraum auch aus dem kontinuierlichen Posenraum, wie er meist für mobile Systeme vorliegt, gezogen werden \cite{Fox2001}. Die Kontrolle über die Partikelanzahl ermöglicht es zudem, den Algorithmus auf die verfügbaren Rechenressourcen abzustimmen \cite{Thrun2001}.\\
Durch die geringen Einschränkungen bezüglich der als Basis verwendeten Wahrscheinlichkeitsverteilungen ist die Monte Carlo Lokalisation in der Lage die globale und lokale Lokalisation mit hoher Genauigkeit zu realisieren \cite{Thrun2005}. Die hohe Effizienz dieses Ansatzes führt dazu, dass er bei der Lokalisation mobiler Systeme der Markov Lokalisation deutlich überlegen ist \cite{Fox2001}.

%\red[Monte Carlo etc. ->Buch\\]
%\red[smartphone lokalisierung thematisieren aber verwendet externe Sensorik zur Positionsbestimmung]

\red[In der Robotik wird die fortlaufende Ableitung der Orientierung und Geschwindigkeit aus Messungen der Raddrehwinkel als Odometrie bezeichnet. \cite{Hertzberg2012}\\]
\red[Wie Quelle für Lokalisationsabsatz referenzieren?]
%\red[Welche Lokalisationsverfahren gibt es. Allgemein und speziell für handgeführte Systeme.]\\


\section{RGB-D Kameras/Tiefenkameras(?)}
Die beschriebenen Verfahren und Algorithmen haben sich besonders im Bereich der 2D Lokalisation bewährt, obwohl sie prinzipiell unabhängig von der Anzahl an Dimensionen sind. Die größer werdende Verbreitung von zugleich kostengünstigen und leistungsfähigen Tiefenkameras wie der Microsoft Kinect\red[Tm] führt jedoch dazu, dass die Anwendungen immer häufiger auch auf 3D Umgebungen erweitert werden. Die Lokalisation in 3D Umgebungen ist besonders dann von Bedeutung, wenn das System sich in mehr als einer Ebene bewegen kann. Die Anzahl der Freiheitsgrade (in der 2D Lokalisation meist drei) steigt dadurch auf sechs an, da die Pose des Systems nun über drei translatorische sowie drei rotatorische Freiheitsgerade beschrieben wird. Neben der Notwendigkeit für geeignete 3D Modellumgebungen kann die Erhöhung der Freiheitsgrade auch dazu führen, dass keine Odometriedaten mehr ermittelt werden können. Dies ist insbesondere bei fliegenden \cite{Huang2011} und hand- oder körpergeführten Systemen \cite{Fallon2012} der Fall. Die Auswertung von Tiefen- und Farbinformationen ermöglicht es jedoch dies durch visuelle Odometrie zu kompensieren \cite{Whelan2013robust}.\\
Auch die Integration in traditionelle Robotersysteme zeigt, dass Tiefenkameras eine sinnvolle Alternative zu bisherigen Sensoren wie Laser-Entfernungsmessern darstellen können \cite{Cunha2011} \cite{Eriksson2012}. Anzumerken ist, dass der beobachtbare Bildbereich bezüglich Distanz und Sichtfeld meist deutlich kleiner ist als bei Laser Sensoren. Die Eignung von Tiefenkameras für Lokalisationsaufgaben ist daher anwendungsabhängig zu überprüfen.\\

%\red[Featurebasierte Lokalisation (RGB-D SLAM, Fovis)\\
%Markerbasierte Lokalisation]\\

\section{Augmented Reality}
Die Überlagerung oder Vereinigung von virtuellen und realen Umgebungen wird als Augmented Reality (AR) bezeichnet \cite{Azuma1997}. Durch eine dreidimensionale Registrierung wird die Interaktion zwischen den Objekten der beiden Welten ermöglicht. Die Umgebung des menschlichen Beobachters wird somit um virtuelle Elemente ergänzt. Neben herkömmlichen und transparenten Bildschirmen \red[quellen? google glass!?] werden insbesondere Projektoren zur Visualisierung der virtuellen Daten eingesetzt. Anwendungen finden sich in der Medizin als Unterstützung von Ärzten in der Chirurgie \cite{Gavaghan2012} \cite{Hoppe2001} oder als Hilfe bei der Kommunikation zwischen Arzt und Patient \cite{Bluteau2005}. Durch Zusatzinformationen kann AR die Interaktionsmöglichkeiten mit Robotersystemen erweitern \cite{DeTommaso2012} oder statische Objekte animieren \cite{Raskar1999}. In Alltagssituationen entsteht durch die Visualisierung von Informationen eine interaktive Benutzerschnittstelle \cite{Linder2010} \cite{Huber2012}.\\
Durch Kombination mehrerer Projektoren können virtuelle Umgebungen auf realen Geometrien erzeugt \cite{Low2001} oder der Interaktionsbereich auf weiter entfernte Oberflächen ausgedehnt werden \cite{Wilson2010}.

%\red[Low - Life-Sized Projector Based Dioramas -> Hausumgebung auf leere Wände projiziert]\\
%\red[Oh - Projektion von Entertainment/Filmen etc. auf Oberflächen]\\
%\red[Raskar - Table-Top AR, Bringing Physical Models to life]\\
%\red[Huber - Lightbeam -> Interaktion mit Projektionen über Alltagsgegenstände]\\
%\red[Linder - LuminAR -> Interaktion mit Projektion in Büroumgebung]\\
%\red[Wilson - Interaktion zwischen mehreren Oberflächen durch Einsatz mehrerer Projektoren und Kameras]

\red[Bimber - Embedded Entertainment with Smart Projectors -> für Ausblick/Limitierungen bzgl. der Projektionsuntergründe]
\red[auch Park - Kontrast erhöhung]
\red[auch Wang - Relighting]\\%
\red[Bimber - Multi-Projector Techniques for Real Time... -> Architectural Projection]\\

%\section{Interaktion}
%\red[Benutzerinteraktion basierend auf der Verwendung von Tiefeninformationen. Hauptsächlicher Ansatz ist die Befehlsvorgabe über Gestensteuerung.]\\
%\red[Welche Formen von Benutzerinteraktion gibt es, besonders bezogen auf die Kinect und Projektionssysteme.\\
%(z.B. Omnitouch)]\\

%\red[Wen - Handgesten zur Interaktion in der Chirurgie]\\



\section{Handgeführte Projektionssysteme}
%\subsection{Lokalisation}
%\red[Ein handgeführtes Scanning System, entwickelt von Mair \textit{et al.} \cite{Mair2010}, fusioniert IMU Daten und  \\]

%\subsection{Projektion}
Die Miniaturisierung der Projektionstechnologie hat zu einer Entwicklung mobilerer, handgeführter Systeme geführt. In den letzen Jahren wurden verschiedene Ansätze untersucht, welche sich mit der Projektion virtueller Zusatzinformationen durch tragbare Systeme befassen. Dabei werden sowohl Miniprojektoren als auch in Smartphones integrierte Projektionssysteme eingesetzt. Im folgenden soll ein Überblick über vorhandene handgeführte Projektionssysteme und ihre Anwendungsbereiche gegeben werden.\\
Die Navigation innerhalb eines Museums wird von Wecker \textit{et al.} \cite{Wecker2013} mittels handgeführter Projektionssysteme durch die Visualisierung von Karten- und Wegdaten unterstützt.\\
Ein System mit ähnlichem Ziel haben Chung \textit{et al.} \cite{Chung2011} entwickelt. Dieses soll dem Anwender bei der Navigation innerhalb von Gebäuden behilflich sein. Ein Miniprojektor wird dabei in Kombination mit einem Smartphone verwendet um zusätzliche Informationen bei der Erkennung von Visitenkarten oder Gebäudeplänen zu visualisieren. Die Funktionalität soll dabei an eine Taschenlampe erinnern, welche die Zusatzinformationen sichtbar macht.\\
Auch Li \textit{et al.} \cite{Li2013} stellen ein handgeführtes Projektionssystem vor, welches im Konzept an eine Taschenlampe angelehnt ist. Durch Projektion von Karten- und Wegdaten auf den Boden vor dem Benutzer wird dieser entlang eines Weges geführt. Im Gegensatz zu Chung \textit{et al.} erfolgt dabei eine kontinuierliche Aktualisierung der Projektion in Abhängigkeit der Position entlang des Weges. Die Lokalisation wurde dabei manuell durch eine Begleitperson vorgenommen.\\
Molyneaux \textit{et al.} \cite{Molyneaux2012} erweitern die Metapher der Taschenlampe und integrieren darüber hinaus eine automatische Lokalisation des handgeführten Projektionssystems. Eine Infrastruktur aus Microsoft Kinect Sensoren erkennt und verfolgt die Systempose und ermöglicht dadurch die verzerrungsfreie Projektion beliebiger Zusatzinformationen innerhalb eines Raumes. Durch Infrarot Kameras am Projektionssystem selbst wird zudem eine Interaktion mit den visualisierten Daten realisiert.\\
Das \textit{SideBySide} Projekt von Willis \textit{et al.} \cite{Willis2011} ermöglicht die Interaktion zwischen Benutzern über handgeführte Projektionssysteme. Jedes System projiziert dabei sowohl ein Bild im sichtbaren Lichtspektrum als auch einen Marker im Infrarot Spektrum. Die Erkennung der Marker durch die Systeme erlaubt das Zusammenspiel der jeweiligen Projektionen der Anwender. Beispielanwendungen finden sich im Austausch von Informationen oder Dateien und in kooperativen Spielen.\\
Einen weiteren Ansatz für kooperative Projektionssysteme liefern Robinson \textit{et al.} \cite{Robinson2012} mit \textit{PicoTales}. Dabei werden handgeführte Projektionssysteme verwendet um gemeinsam animierte Videos zu erstellen. Die Lokalisation erfolgt dabei nach einem Kalibrierungsverfahren über das Aufzeichnen von Bewegungsdaten durch eine inertiale Messeinheit. Die Auswertung und Fusionierung zu einem gemeinsamen Video erfolgt nach Ende der Interaktion über einen separaten Computer.\\
Das von Harrison \textit{et al.\ }\cite{Harrison2011} entwickelte \textit{Omnitouch} ist ein körpergeführtes System, welches die Projektion grafischer Benutzeroberflächen auf typische im Alltag vorhandene Oberflächen ermöglicht. Das System verfügt neben einem Projektor auch über eine Tiefenkamera zur Detektion von Benutzereingaben. Dadurch wird die Funktionalität von Touchscreens abgebildet und es können typische Anwendungen implementiert werden, die sonst beispielsweise auf Smartphones oder Tablets genutzt werden.\\
Ein vergleichbarer Aufbau wird von Tan \textit{et al.} \cite{Tan2013} verwendet um virtuelle Modelldaten auf reale Modelle zu projizieren. Dies ermöglicht die korrekte Darstellung der virtuellen Daten auch bei einem Wechsel der Beobachterperspektive. Die Lokalisation des Systems erfolgt dabei anhand des Abgleichs zwischen Modell und Sensordaten.\\

\red[Abbildungen der Systeme?\\]

%Warum der Ansatz?
Wenige der handgeführten Systeme ermitteln ihre Pose innerhalb der Umgebung. Häufig ist lediglich die relative Pose bezüglich definierter Oberflächen oder Objekte Bestandteil der Betrachtung. Systeme, welche eine globale Lokalisation erfordern, verwenden dagegen entweder manuelle oder auf externen Sensoren basierende Lokalisationsverfahren. Die Selbstlokalisation mobiler Systeme ist allerdings seit einiger Zeit Forschungsthema und es existiert eine Vielzahl von Ansätzen um eine zuverlässige Lokalisation in zwei- und dreidimensionalen bekannten Umgebungen zu erreichen. Eine Übertragung auf den Anwendungsbereich handgeführter Projektionssysteme hat jedoch bisher nicht stattgefunden.\\
Die vorliegende Arbeit soll die Lücke zwischen der Lokalisation mobiler Systeme und der projektorbasierten \textit{Augmented Reality} schließen. Die Anwendungsgebiete werden vereint, indem die Selbstlokalisation eines handgeführten \kps{s} realisiert und darauf aufbauend die Projektion visueller Zusatzinformationen in der realen Umgebung ermöglicht wird.
%\red[Verschiedene handgeführte Systeme, welche jedoch entweder auf manueller, externer Lokalisation, oder markerbasierter Lokalisation beruhen. Bei anderen Systemen wird lediglich eine Ausrichtung bzgl der Projektionsoberfläche durchgeführt. Selbstlokalisation ohne Hilfsmittel (externe Sensorik, Marker in Form von KArten o.ä.) bisher nicht behandelt. Lokalisation von mobilen Systemen allerdings seit einiger Zeit Forschungsthema, neuer ist jedoch die 3D Lokalisation. System soll die Brücke bilden zwischen mobilen, autonomen Lokalisationssystemen und handgeführten Projektionssystemen\\]
\red[OHNE HILFSMITTEL nochmal aufgreifen wenn alternative Lokalisation über Muster o.ä. aufgeführt wird]

\red[Tan - iSarProjection -> Handgeführtes System, Aufbau sehr ähnlich Kinpro, Lokalisation aber anhand der Modelle auf die dann projiziert wird, eher wie David]\\

\red[Welche Systeme gibt es zur Projektion von (Modell-)Daten.]\\

%\includesvgnew[1]{test}
